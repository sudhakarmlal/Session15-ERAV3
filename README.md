# Session15-ERAV3

SMOLLM2 Model  Configurations:

MultiHead Latent Attention used to reduce the parameters count
Mixture of Experts is used to train the model in with different experts
Model Parameters

Vocab Size : 49512

Sequennce Length : 256

Batch Size : 4

Accumulation Steps : 8

Dimension : 576

Intermediate Size : 1536

No Of Layers : 30

No of Heads : 9

Compres Ratio : 3

No Of Experts : 8
No Of Shared Experts : 1
Top-K Experts : 2

##Training Logs:
